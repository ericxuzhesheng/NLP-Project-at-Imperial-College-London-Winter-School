{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation in Biomedical Domain\n",
    "\n",
    "Before you start, please make sure you have read this notebook. You are encouraged to follow the recommendations but you are also free to develop your own solution from scratch.\n",
    "\n",
    "## Marking Scheme\n",
    "\n",
    "- Biomedical imaging project: 40%\n",
    "  - 20%: accuracy of the final model on the test set\n",
    "  - 20%: rationale of model design and final report\n",
    "- Natural language processing project: 40%\n",
    "  - 30%: completeness of the project\n",
    "  - 10%: final report\n",
    "- Presentation skills and team work: 20%\n",
    "\n",
    "This project forms 40\\% of the total score for summer/winter school. The marking scheme of each part of this project is provided below with a cap of 100\\%.\n",
    "\n",
    "You are allowed to use open source libraries as long as the libraries are properly cited in the code and final report. The usage of third-party code without proper reference will be treated as plagiarism, which will not be tolerated.\n",
    "\n",
    "You are encouraged to develop the algorithms by yourselves (without using third-party code as much as possible). We will factor such effort into the marking process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites\n",
    "\n",
    "Recommended environment\n",
    "\n",
    "- Python 3.7 or newer\n",
    "- Free disk space: 100GB\n",
    "\n",
    "Download the data\n",
    "\n",
    "```sh\n",
    "# navigate to the data folder\n",
    "cd data\n",
    "\n",
    "# download the data file\n",
    "# which is also available at https://www.semanticscholar.org/cord19/download\n",
    "wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-07-26/document_parses.tar.gz\n",
    "\n",
    "# decompress the file which may take several minutes\n",
    "tar -xf document_parses.tar.gz\n",
    "\n",
    "# which creates a folder named document_parses\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (20%): Parse the Data\n",
    "\n",
    "The JSON files are located in two sub-folders in `document_parses`. You will need to scan all JSON files and extract content (i.e. `string`) from relevant fields (e.g. body text, abstract, titles).\n",
    "\n",
    "You are encouraged to extract full article text from body text if possible. If the hardware resource is limited, you can extract from abstract or titles as alternatives.\n",
    "\n",
    "Note: The number of JSON files is around 425k so it may take more than 10 minutes to parse all documents.\n",
    "\n",
    "For more information about the dataset: https://www.semanticscholar.org/cord19/download\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A list of text (`string`) extracted from JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 打开并读取 metadata.csv 文件\n",
    "with open(r\"all_sources_metadata_2020-03-13.csv\", encoding=\"utf-8\") as f_in:\n",
    "    reader = csv.DictReader(f_in)\n",
    "\n",
    "    # 打开 txt 文件以写入\n",
    "    with open(\"titles_abstracts.txt\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for row in reader:\n",
    "            title = row[\"title\"].strip() if row[\"title\"] else \" \"\n",
    "            abstract = row[\"abstract\"].strip() if row[\"abstract\"] else \" \"\n",
    "\n",
    "            # 合并标题和摘要\n",
    "            text = f\"{title}\\n{abstract}\\n\\n\"\n",
    "\n",
    "            # 写入文件\n",
    "            f_out.write(text)\n",
    "\n",
    "print(\"标题和摘要已成功存入 titles_abstracts.txt 文件！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (30%): Tokenization\n",
    "\n",
    "Traverse the extracted text and segment the text into words (or tokens).\n",
    "\n",
    "The following tracks can be developed in independentely. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- Tokenizer(s) that is able to tokenize any input text.\n",
    "\n",
    "Note: Because of the computation complexity of tokenizers, it may take hours/days to process all documents. Which tokenizer is more efficient? Any idea to speedup?\n",
    "\n",
    "### Track 2.1 (10%): Use split()\n",
    "\n",
    "Use the standard `split()` by Python.\n",
    "\n",
    "### Track 2.2 (10%): Use NLTK or SciSpaCy\n",
    "\n",
    "NLTK tokenizer: https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "SciSpaCy: https://github.com/allenai/scispacy\n",
    "\n",
    "Note: You may need to install NLTK and SpaCy so please refer to their websites for installation instructions.\n",
    "\n",
    "### Track 2.3 (10%): Use Byte-Pair Encoding (BPE)\n",
    "\n",
    "Byte-Pair Encoding (BPE): https://huggingface.co/transformers/tokenizer_summary.html\n",
    "\n",
    "Note: You may need to install Huggingface's transformers so please refer to its website for installation instructions.\n",
    "\n",
    "### Track 2.4 (Bonus +5%): Build new Byte-Pair Encoding (BPE)\n",
    "\n",
    "This track may be dependent on track 2.3.\n",
    "\n",
    "The above pre-built tokenization methods may not be suitable for biomedical domain as the words/tokens (e.g. diseases, sympotoms, chemicals, medications, phenotypes, genotypes etc.) can be very different from the words/tokens commonly used in daily life. Can you build and train a new BPE model for biomedical domain in particular?\n",
    "\n",
    "### Open Question (Optional):\n",
    "\n",
    "- What are the pros and cons of the above tokenizers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./titles_abstracts.txt\"\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 2.1: Use split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization_1(content):\n",
    "    words = []\n",
    "    lines = content.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        words.extend(re.split(r\"\\W+\", line))  # 使用正则表达式分割\n",
    "    # 过滤掉空字符串，确保抽取的都是有效单词\n",
    "    words = [word for word in words if word]\n",
    "\n",
    "    # 使用 Counter 类统计单词出现次数，并根据出现次数进行加权抽样\n",
    "    word_counts = Counter(words)\n",
    "    weighted_sample = random.choices(\n",
    "        list(word_counts.keys()), weights=word_counts.values(), k=20\n",
    "    )\n",
    "    print(\"Weighted sample:\", \" \".join(weighted_sample))\n",
    "\n",
    "    result = \" \".join(words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 2.2: Use NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization_2(content):\n",
    "    # 分句\n",
    "    sents = sent_tokenize(content)\n",
    "\n",
    "    interpunctuations = [\n",
    "        \",\",\n",
    "        \".\",\n",
    "        \":\",\n",
    "        \";\",\n",
    "        \"?\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"[\",\n",
    "        \"]\",\n",
    "        \"&\",\n",
    "        \"!\",\n",
    "        \"*\",\n",
    "        \"@\",\n",
    "        \"#\",\n",
    "        \"$\",\n",
    "        \"%\",\n",
    "        \"/\",\n",
    "        \"\\\\\",\n",
    "        \"—\",\n",
    "    ]  # 标点符号\n",
    "\n",
    "    words = []\n",
    "    for sent in sents:\n",
    "        sent_t = word_tokenize(sent)  # 分词\n",
    "        sent_t = [\n",
    "            word.lower() for word in sent_t if word not in interpunctuations\n",
    "        ]  # 移除标点符号\n",
    "        words.append(sent_t)\n",
    "\n",
    "    # 保存分词结果\n",
    "    result = []\n",
    "    for sent in words:\n",
    "        result.append(\" \".join(sent))\n",
    "    with open(\"./nlkt_tokenized.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 2.3: Use Byte-Pair Encoding (BPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization_3(content):\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # 按段落分割文本（避免超长输入）\n",
    "    max_length = 512\n",
    "    result = []\n",
    "\n",
    "    for i in range(0, len(content), max_length):\n",
    "        chunk = content[i : i + max_length]\n",
    "        tokens = bert_tokenizer.tokenize(chunk)\n",
    "        result.append(\" \".join(tokens))  # 以空格连接 token\n",
    "\n",
    "    # 保存分词结果\n",
    "    with open(\"bert_tokenized.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(result))\n",
    "    print(\"BERT 分词结果已保存至 `bert_tokenized.txt`\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 2.4: Build new Byte-Pair Encoding (BPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization_4(content):\n",
    "    # 训练 BPE 模型\n",
    "    print(\"Training BPE tokenizer...\")\n",
    "    bpe_tokenizer = Tokenizer(models.BPE())\n",
    "    bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    trainer = trainers.BpeTrainer(special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"])\n",
    "    bpe_tokenizer.train(files=[\"titles_abstracts.txt\"], trainer=trainer)\n",
    "\n",
    "    # 保存 BPE 训练模型\n",
    "    bpe_tokenizer.save(\"biomedical_bpe.json\")\n",
    "\n",
    "    # 重新加载 BPE tokenizer 并进行分词\n",
    "    bpe_tokenizer = Tokenizer.from_file(\"biomedical_bpe.json\")\n",
    "    bpe_tokens = bpe_tokenizer.encode(content)\n",
    "\n",
    "    # 保存 BPE 分词结果\n",
    "    with open(\"bpe_tokenized.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(bpe_tokens.tokens))  # 以空格连接 token\n",
    "\n",
    "    print(\"BPE 分词结果已保存至 `bpe_tokenized.txt`\")\n",
    "\n",
    "    return bpe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (30%): Build Word Representations\n",
    "\n",
    "Build word representations for each extracted word. If the hardware resource is limited, you may limit the vocabulary size up to 10k words/tokens (or even smaller) and the dimension of representations up to 256.\n",
    "\n",
    "The following tracks can be developed independently. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "### Track 3.1 (15%): Use N-gram Language Modeling\n",
    "\n",
    "N-gram Language Modeling is to predict a target word by using `n` words from previous context. Specifically,\n",
    "\n",
    "$P(w_i | w_{i-1}, w_{i-2}, ..., w_{i-n+1})$\n",
    "\n",
    "For example, given a sentence, `\"the main symptoms of COVID-19 are fever and cough\"`, if $n=7$, we use previous context `[\"the\", \"main\", \"symptoms\", \"of\", \"COVID-19\", \"are\"]` to predict the next word `\"fever\"`.\n",
    "\n",
    "More to read: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.2 (15%): Use Skip-gram with Negative Sampling\n",
    "\n",
    "In skip-gram, we use a central word to predict its context. Specifically,\n",
    "\n",
    "$P(w_{c-m}, ... w_{c-1}, w_{c+1}, ..., w_{c+m} | w_c)$\n",
    "\n",
    "As the learning objective of skip-gram is computational inefficient (summation of entire vocabulary $|V|$), negative sampling is commonly applied to accelerate the training.\n",
    "\n",
    "In negative sampling, we randomly select one word from the context as a positive sample, and randomly select $K$ words from the vocabulary as negative samples. As a result, the learning objective is updated to\n",
    "\n",
    "$L = -\\log\\sigma(u^T_{t} v_c) - \\sum_{k=1}^K\\log\\sigma(-u^T_k v_c)$, where $u_t$ is the vector embedding of positive sample from context, $u_k$ are the vector embeddings of negative samples, $v_c$ is the vector embedding of the central word, $\\sigma$ refers to the sigmoid function.\n",
    "\n",
    "More to read http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf section 4.3 and 4.4\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.3 (Bonus +5%): Use Contextualised Word Representation by Masked Language Model (MLM)\n",
    "\n",
    "BERT introduces a new language model for pre-training named Masked Language Model (MLM). The advantage of MLM is that the word representations by MLM will be contextualised.\n",
    "\n",
    "For example, \"stick\" may have different meanings in different context. By N-gram language modeling and word2vec (skip-gram, CBOW), the word representation of \"stick\" is fixed regardless of its context. However, MLM will learn the representation of \"stick\" dynamatically based on context. In other words, \"stick\" will have different representations in different context by MLM.\n",
    "\n",
    "More to read: http://jalammar.github.io/illustrated-bert/ and https://arxiv.org/pdf/1810.04805.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- An algorithm that is able to generate contextualised representation in real time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 3.1: Use N-gram Language Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, n, vocab_size=10000, embedding_dim=100):\n",
    "        self.n = n\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 词表相关\n",
    "        self.vocab = {\"<UNK>\", \"<START>\", \"<END>\"}\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "\n",
    "        # N-gram统计\n",
    "        self.ngram_counts = defaultdict(lambda: defaultdict(float))\n",
    "        self.context_counts = defaultdict(float)\n",
    "\n",
    "        # 词向量相关\n",
    "        self.word_embeddings = None\n",
    "        self.context_embeddings = None\n",
    "\n",
    "        # 平滑参数\n",
    "        self.alpha = 0.1\n",
    "\n",
    "    def build_vocab(self, text):\n",
    "        \"\"\"构建词表\"\"\"\n",
    "        words = text.split()\n",
    "        word_counts = Counter(words)\n",
    "        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_words = top_words[: self.vocab_size - 3]  # 留出特殊标记的空间\n",
    "\n",
    "        self.vocab.update(word for word, _ in top_words)\n",
    "        self.word_to_id = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.id_to_word = {idx: word for word, idx in self.word_to_id.items()}\n",
    "\n",
    "    def build_cooccurrence_matrix(self, text):\n",
    "        \"\"\"构建共现矩阵\"\"\"\n",
    "        words = [\"<START>\"] + text.split() + [\"<END>\"]\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        # 初始化稀疏矩阵的数据结构\n",
    "        rows, cols, data = [], [], []\n",
    "\n",
    "        # 统计共现\n",
    "        for i in range(len(words) - self.n + 1):\n",
    "            context_words = words[i : i + self.n - 1]\n",
    "            target_word = words[i + self.n - 1]\n",
    "\n",
    "            # 获取单词ID\n",
    "            context_ids = [\n",
    "                self.word_to_id.get(w, self.word_to_id[\"<UNK>\"]) for w in context_words\n",
    "            ]\n",
    "            target_id = self.word_to_id.get(target_word, self.word_to_id[\"<UNK>\"])\n",
    "\n",
    "            # 更新N-gram统计\n",
    "            context = tuple(context_ids)\n",
    "            self.ngram_counts[context][target_id] += 1\n",
    "            self.context_counts[context] += 1\n",
    "\n",
    "            # 更新共现统计\n",
    "            for context_id in context_ids:\n",
    "                rows.append(context_id)\n",
    "                cols.append(target_id)\n",
    "                data.append(1.0)\n",
    "\n",
    "        # 构建稀疏矩阵\n",
    "        M = csr_matrix((data, (rows, cols)), shape=(vocab_size, vocab_size))\n",
    "\n",
    "        # 计算条件概率 P(wj|wi)\n",
    "        row_sums = M.sum(axis=1).A.flatten()\n",
    "        row_sums[row_sums == 0] = 1  # 避免除零\n",
    "        M = csr_matrix(M / row_sums[:, np.newaxis])\n",
    "\n",
    "        return M\n",
    "\n",
    "    def factorize_matrix(self, M):\n",
    "        \"\"\"使用SVD分解共现矩阵\"\"\"\n",
    "        # Perform Singular Value Decomposition\n",
    "        U, S, Vt = svds(M, k=self.embedding_dim)\n",
    "\n",
    "        # Scale singular values\n",
    "        S_sqrt = np.sqrt(S)\n",
    "        self.word_embeddings = U * S_sqrt  # Scale each column of U\n",
    "        self.context_embeddings = Vt.T * S_sqrt  # Scale each row of Vt.T\n",
    "\n",
    "    def train(self, text):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        # 构建词表\n",
    "        self.build_vocab(text)\n",
    "\n",
    "        # 构建并分解共现矩阵\n",
    "        M = self.build_cooccurrence_matrix(text)\n",
    "        self.factorize_matrix(M)\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        \"\"\"获取词向量\"\"\"\n",
    "        word_id = self.word_to_id.get(word, self.word_to_id[\"<UNK>\"])\n",
    "        return self.word_embeddings[word_id]\n",
    "\n",
    "    def get_context_vector(self, context):\n",
    "        \"\"\"获取上下文向量\"\"\"\n",
    "        context_words = context.split()[-self.n + 1 :]\n",
    "        context_ids = [\n",
    "            self.word_to_id.get(w, self.word_to_id[\"<UNK>\"]) for w in context_words\n",
    "        ]\n",
    "        return np.mean([self.context_embeddings[i] for i in context_ids], axis=0)\n",
    "\n",
    "    def predict_next(self, context):\n",
    "        \"\"\"预测下一个词\"\"\"\n",
    "        # 获取上下文的向量表示\n",
    "        context_vec = self.get_context_vector(context)\n",
    "\n",
    "        # 计算与所有词的相似度\n",
    "        similarities = np.dot(self.word_embeddings, context_vec)\n",
    "\n",
    "        # 获取最相似的词\n",
    "        best_id = np.argmax(similarities)\n",
    "        best_word = self.id_to_word[best_id]\n",
    "\n",
    "        # 计算概率（使用softmax）\n",
    "        exp_similarities = np.exp(similarities - np.max(similarities))\n",
    "        probability = exp_similarities[best_id] / exp_similarities.sum()\n",
    "\n",
    "        return best_word, float(probability)\n",
    "\n",
    "    def generate(self, seed, length=10):\n",
    "        \"\"\"生成文本\"\"\"\n",
    "        current = [\"<START>\"] + seed.split()\n",
    "        result = current.copy()\n",
    "\n",
    "        for _ in range(length):\n",
    "            context = \" \".join(current[-(self.n - 1) :])\n",
    "            next_word, _ = self.predict_next(context)\n",
    "\n",
    "            if next_word == \"<END>\":\n",
    "                break\n",
    "\n",
    "            result.append(next_word)\n",
    "            current = result[-self.n + 1 :]\n",
    "\n",
    "        return \" \".join(result[1:])  # 去掉<START>标记\n",
    "\n",
    "    def get_similar_words(self, word, k=5):\n",
    "        \"\"\"找到最相似的k个词\"\"\"\n",
    "        word_vec = self.get_word_vector(word)\n",
    "        similarities = np.dot(self.word_embeddings, word_vec)\n",
    "        top_k = np.argsort(similarities)[-k - 1 : -1][::-1]  # 不包括词本身\n",
    "        return [(self.id_to_word[i], similarities[i]) for i in top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Representations_1(result):\n",
    "    # 使用分词后的文本结果\n",
    "    tokenized_text = result\n",
    "\n",
    "    model = NGramLanguageModel(3)\n",
    "    model.train(tokenized_text)\n",
    "\n",
    "    # 输出词向量\n",
    "    for word, vector in model.get_word_vector().items():\n",
    "        print(f\"Word: {word}, Vector: {vector[:5]}...\")  # 仅显示前5个维度\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 3.2: Use Skip-gram with Negative Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramWithNegativeSampling:\n",
    "    def __init__(self, vocab_size, vector_dim, window_size, negative_samples):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_dim = vector_dim\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "        # 词向量随机初始化\n",
    "        self.word_vectors = np.random.uniform(-0.5, 0.5, (vocab_size, vector_dim))\n",
    "        self.context_vectors = np.random.uniform(-0.5, 0.5, (vocab_size, vector_dim))\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        # 激活函数\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _loss(self, central_vec, context_vec, negative_vecs):\n",
    "        # 损失函数\n",
    "        positive_score = np.dot(context_vec, central_vec)\n",
    "        positive_loss = -np.log(self._sigmoid(positive_score))\n",
    "\n",
    "        negative_scores = np.dot(negative_vecs, central_vec)\n",
    "        negative_loss = -np.sum(np.log(self._sigmoid(-negative_scores)))\n",
    "\n",
    "        return positive_loss + negative_loss\n",
    "\n",
    "    def _gradient_update(self, central_vec, context_vec, negative_vecs, learning_rate):\n",
    "        # 正样本梯度\n",
    "        positive_score = self._sigmoid(np.dot(context_vec, central_vec))\n",
    "        positive_grad = (positive_score - 1) * context_vec\n",
    "\n",
    "        # 负样本梯度\n",
    "        negative_scores = self._sigmoid(np.dot(negative_vecs, central_vec))\n",
    "        negative_grad = np.sum(negative_scores[:, np.newaxis] * negative_vecs, axis=0)\n",
    "\n",
    "        # 更新向量\n",
    "        central_vec -= learning_rate * (positive_grad + negative_grad)\n",
    "        context_vec -= learning_rate * (positive_score - 1) * central_vec\n",
    "\n",
    "        for neg_vec in negative_vecs:\n",
    "            neg_vec -= learning_rate * negative_scores * central_vec\n",
    "\n",
    "    def train(self, dataset, epochs, learning_rate):\n",
    "        # 梯度下降训练模型\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for sentence in dataset:\n",
    "                for i, central_word in enumerate(sentence):\n",
    "                    # 设置窗\n",
    "                    start = max(0, i - self.window_size)\n",
    "                    end = min(len(sentence), i + self.window_size + 1)\n",
    "\n",
    "                    # 获取句子\n",
    "                    context_words = [sentence[j] for j in range(start, end) if j != i]\n",
    "\n",
    "                    for context_word in context_words:\n",
    "                        # 正样本\n",
    "                        central_vec = self.word_vectors[central_word]\n",
    "                        context_vec = self.context_vectors[context_word]\n",
    "\n",
    "                        # 负样本\n",
    "                        negative_indices = random.sample(\n",
    "                            [\n",
    "                                idx\n",
    "                                for idx in range(self.vocab_size)\n",
    "                                if idx not in sentence\n",
    "                            ],\n",
    "                            self.negative_samples,\n",
    "                        )\n",
    "                        negative_vecs = self.context_vectors[negative_indices]\n",
    "\n",
    "                        # 计算损失函数\n",
    "                        loss = self._loss(central_vec, context_vec, negative_vecs)\n",
    "                        total_loss += loss\n",
    "\n",
    "                        # 更新梯度\n",
    "                        self._gradient_update(\n",
    "                            central_vec, context_vec, negative_vecs, learning_rate\n",
    "                        )\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "    def get_word_vectors(self):\n",
    "        # 获取词向量\n",
    "        return self.word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Representations_2(result):\n",
    "    # 预处理句子\n",
    "    tokens = [sent.split(\" \") for sent in result]\n",
    "\n",
    "    # 设置模型参数\n",
    "    model = Word2Vec(\n",
    "        sentences=tokens[:20_000],  # 输入预处理后的句子\n",
    "        vector_size=256,  # 嵌入向量的维度\n",
    "        window=5,  # 窗口大小（前后最多考虑 5 个词）\n",
    "        sg=1,  # Skip-gram 模式（sg=1 表示 Skip-gram；sg=0 表示 CBOW）\n",
    "        negative=10,  # 负采样数量（每个正样本对应的负样本数量）\n",
    "        min_count=2,  # 忽略出现次数少于 2 次的词\n",
    "        workers=4,  # 使用的 CPU 核心数\n",
    "        epochs=10,  # 训练的迭代次数\n",
    "    )\n",
    "\n",
    "    # 输出词向量\n",
    "    vocab = model.wv.index_to_key\n",
    "    for word in vocab:\n",
    "        vector = model.wv[word]\n",
    "        print(f\"Word: {word}, Vector: {vector[:5]}...\")  # 仅显示前5个维度\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 3.3: Use Contextualised Word Representation by Masked Language Model (MLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Representations_3():\n",
    "    # 加载 BERT（Masked Language Model 版本）\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # 配置 LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,  # 低秩适配维度\n",
    "        lora_alpha=32,  # LoRA alpha 参数\n",
    "        lora_dropout=0.1,  # Dropout 概率\n",
    "        target_modules=[\"query\", \"value\"],  # 适配 transformer 的 Q/V 矩阵\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())  # 所有参数\n",
    "    trainable_params = sum(\n",
    "        p.numel() for p in model.parameters() if p.requires_grad\n",
    "    )  # 仅可训练参数\n",
    "\n",
    "    print(f\"🔹 模型总参数量: {total_params:,}\")\n",
    "    print(f\"🔸 可训练参数量: {trainable_params:,}\")\n",
    "    print(f\"🔹 冻结参数量: {total_params - trainable_params:,}\")\n",
    "\n",
    "    # 读取文本数据\n",
    "    with open(\"titles_abstracts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    # 构造 Hugging Face 数据集\n",
    "    dataset = Dataset.from_dict({\"text\": lines})\n",
    "\n",
    "    # 拆分数据集\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    # 预处理：Tokenization + MLM 掩码\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256\n",
    "        )\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # 数据 collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=True,  # 进行 MLM 训练\n",
    "        mlm_probability=0.15,  # 15% 的 token 被 mask\n",
    "    )\n",
    "\n",
    "    # 训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./bert_lora_finetuned\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=500,\n",
    "    )\n",
    "\n",
    "    # 训练器\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.train()\n",
    "\n",
    "    # 设为评估模式\n",
    "    model.eval()\n",
    "\n",
    "    # 读取文本并进行 tokenization\n",
    "    with open(\"titles_abstracts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()[:500]\n",
    "\n",
    "    # 逐行处理\n",
    "    all_embeddings = []\n",
    "    for line in lines:\n",
    "        inputs = tokenizer(\n",
    "            line.strip(),\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)  # ✅ 直接调用 BERT\n",
    "\n",
    "        embeddings = outputs.logits  # ✅ 提取 token 预测结果\n",
    "        all_embeddings.append(embeddings.squeeze(0))\n",
    "\n",
    "    # 打印信息\n",
    "    print(f\"嵌入总数: {len(all_embeddings)}\")\n",
    "    print(f\"单个嵌入的形状: {all_embeddings[0].shape}\")\n",
    "\n",
    "    # 保存 token 嵌入\n",
    "    torch.save(all_embeddings, \"bert_lora_finetuned_embeddings.pt\")\n",
    "    print(\"LoRA 微调后的 token 嵌入已保存！\")\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (20%): Explore the Word Representations\n",
    "\n",
    "The following tracks can be finished independently. You are encouraged to divide workload to each team member.\n",
    "\n",
    "### Track 4.1 (5%): Visualise the word representations by t-SNE\n",
    "\n",
    "t-SNE is an algorithm to reduce dimentionality and commonly used to visualise high-dimension vectors. Use t-SNE to visualise the word representations. You may visualise up to 1000 words as t-SNE is highly computationally complex.\n",
    "\n",
    "More about t-SNE: https://lvdmaaten.github.io/tsne/\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram by t-SNE based on representations of up to 1000 words.\n",
    "\n",
    "### Track 4.2 (5%): Visualise the Word Representations of Biomedical Entities by t-SNE\n",
    "\n",
    "Instead of visualising the word representations of the entire vocabulary (or 1000 words that are selected at random), visualise the word representations of words which are biomedical entities. For example, fever, cough, diabetes etc. Based on the category of those biomedical entities, can you assign different colours to the entities and see if the entities from the same category can be clustered by t-SNE? For example, sinusitis and cough are both respirtory diseases so they should be assigned with the same colour and ideally their representations should be close to each other by t-SNE. Another example, Alzheimer and headache are neuralogical diseases which should be assigned by another colour.\n",
    "\n",
    "Examples of biomedial ontology: https://www.ebi.ac.uk/ols/ontologies/hp and https://en.wikipedia.org/wiki/International_Classification_of_Diseases\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram with colours by t-SNE based on representations of biomedical entities.\n",
    "\n",
    "### Track 4.3 (5%): Co-occurrence\n",
    "\n",
    "- What are the biomedical entities which frequently co-occur with COVID-19 (or coronavirus)?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Track 4.4 (5%): Semantic Similarity\n",
    "\n",
    "- What are the biomedical entities which have closest semantic similarity COVID-19 (or coronavirus) based on word representations?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Open Question (Optional): What else can you discover?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from random import sample\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define BME dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_BME = {\n",
    "    \"disease\": [\n",
    "        \"asthma\",\n",
    "        \"bronchitis\",\n",
    "        \"cancer\",\n",
    "        \"cirrhosis\",\n",
    "        \"disease\",\n",
    "        \"fibrosis\",\n",
    "        \"hypertension\",\n",
    "        \"syndrome\",\n",
    "        \"tuberculosis\",\n",
    "    ],\n",
    "    \"symptom\": [\"chills\", \"cough\", \"dyspnea\", \"fatigue\", \"fever\", \"headache\", \"nausea\"],\n",
    "    \"virus\": [\n",
    "        \"coronavirus\",\n",
    "        \"hepatitis\",\n",
    "        \"herpesvirus\",\n",
    "        \"hiv\",\n",
    "        \"influenza\",\n",
    "        \"norovirus\",\n",
    "        \"rabies\",\n",
    "    ],\n",
    "    \"bacteria\": [\"listeria\", \"mycobacterium\", \"salmonella\", \"streptococcus\"],\n",
    "    \"protein\": [\n",
    "        \"antibody\",\n",
    "        \"cytokine\",\n",
    "        \"enzyme\",\n",
    "        \"hemoglobin\",\n",
    "        \"interleukin\",\n",
    "        \"protein\",\n",
    "        \"receptor\",\n",
    "    ],\n",
    "    \"drug\": [\n",
    "        \"antibiotic\",\n",
    "        \"antiviral\",\n",
    "        \"chloroquine\",\n",
    "        \"oseltamivir\",\n",
    "        \"remdesivir\",\n",
    "        \"vaccine\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "kimi_BME = {\n",
    "    \"viruses\": [\n",
    "        \"virus\",\n",
    "        \"coronavirus\",\n",
    "        \"covid-19\",\n",
    "        \"influenza\",\n",
    "        \"2019-ncov\",\n",
    "        \"sars-cov-2\",\n",
    "        \"h5n1\",\n",
    "        \"mers-cov\",\n",
    "        \"norovirus\",\n",
    "        \"hiv-1\",\n",
    "        \"h1n1\",\n",
    "    ],\n",
    "    \"diseases\": [\n",
    "        \"infection\",\n",
    "        \"disease\",\n",
    "        \"pneumonia\",\n",
    "        \"syndrome\",\n",
    "        \"mortality\",\n",
    "        \"cancer\",\n",
    "        \"pathogen\",\n",
    "        \"illness\",\n",
    "    ],\n",
    "    \"proteins\": [\n",
    "        \"protein\",\n",
    "        \"proteins\",\n",
    "        \"antibodies\",\n",
    "        \"spike\",\n",
    "        \"glycoprotein\",\n",
    "        \"capsid\",\n",
    "        \"membrane\",\n",
    "        \"antigen\",\n",
    "        \"antigens\",\n",
    "        \"epitopes\",\n",
    "    ],\n",
    "    \"genes\": [\n",
    "        \"gene\",\n",
    "        \"genes\",\n",
    "        \"genome\",\n",
    "        \"rna\",\n",
    "        \"dna\",\n",
    "        \"receptor\",\n",
    "        \"genetic\",\n",
    "        \"mutation\",\n",
    "        \"mutations\",\n",
    "    ],\n",
    "    \"cells\": [\n",
    "        \"cells\",\n",
    "        \"cell\",\n",
    "        \"host\",\n",
    "        \"cellular\",\n",
    "        \"blood\",\n",
    "        \"serum\",\n",
    "        \"tissue\",\n",
    "        \"antibody\",\n",
    "        \"antibodies\",\n",
    "    ],\n",
    "    \"treatments\": [\n",
    "        \"treatment\",\n",
    "        \"therapy\",\n",
    "        \"vaccine\",\n",
    "        \"vaccines\",\n",
    "        \"antiviral\",\n",
    "        \"care\",\n",
    "        \"diagnosis\",\n",
    "        \"diagnostic\",\n",
    "        \"drug\",\n",
    "        \"drugs\",\n",
    "    ],\n",
    "    \"epidemiology\": [\n",
    "        \"outbreak\",\n",
    "        \"epidemic\",\n",
    "        \"pandemic\",\n",
    "        \"transmission\",\n",
    "        \"quarantine\",\n",
    "        \"prevention\",\n",
    "        \"public\",\n",
    "        \"surveillance\",\n",
    "        \"population\",\n",
    "        \"spread\",\n",
    "    ],\n",
    "    \"research\": [\n",
    "        \"study\",\n",
    "        \"analysis\",\n",
    "        \"data\",\n",
    "        \"research\",\n",
    "        \"model\",\n",
    "        \"models\",\n",
    "        \"sequencing\",\n",
    "        \"clinical\",\n",
    "        \"review\",\n",
    "        \"findings\",\n",
    "    ],\n",
    "    \"symptoms\": [\n",
    "        \"symptoms\",\n",
    "        \"fever\",\n",
    "        \"cough\",\n",
    "        \"dyspnea\",\n",
    "        \"fatigue\",\n",
    "        \"headache\",\n",
    "        \"sore throat\",\n",
    "        \"shortness of breath\",\n",
    "    ],\n",
    "    \"methods\": [\n",
    "        \"methods\",\n",
    "        \"assay\",\n",
    "        \"pcr\",\n",
    "        \"rt-pcr\",\n",
    "        \"sequencing\",\n",
    "        \"imaging\",\n",
    "        \"tools\",\n",
    "        \"test\",\n",
    "        \"tests\",\n",
    "        \"diagnostic\",\n",
    "    ],\n",
    "    \"public_health\": [\n",
    "        \"health\",\n",
    "        \"public\",\n",
    "        \"hospital\",\n",
    "        \"hospitals\",\n",
    "        \"healthcare\",\n",
    "        \"prevention\",\n",
    "        \"quarantine\",\n",
    "        \"epidemiology\",\n",
    "        \"surveillance\",\n",
    "    ],\n",
    "    \"miscellaneous\": [\n",
    "        \"cases\",\n",
    "        \"patients\",\n",
    "        \"human\",\n",
    "        \"novel\",\n",
    "        \"background\",\n",
    "        \"severity\",\n",
    "        \"risk\",\n",
    "        \"control\",\n",
    "        \"management\",\n",
    "        \"response\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 4.1: Visualise the word representations by t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exploration_1(model, BME=kimi_BME, max_words=1000, seed=1022):\n",
    "    random.seed(seed)\n",
    "    # 提取词汇表和对应的词向量\n",
    "    BMEs = [word for words in list(BME.values()) for word in words]\n",
    "    vocab = model.wv.index_to_key\n",
    "    vocab_1 = list(filter(lambda x: x in BMEs, vocab))\n",
    "    vocab_2 = sample(vocab, max_words - len(vocab_1))\n",
    "\n",
    "    vocab = vocab_1 + vocab_2  # 提取词汇表\n",
    "    word_vectors = model.wv[vocab]  # 提取词向量\n",
    "\n",
    "    # 限制最多可视化的词数量\n",
    "    if len(vocab) > max_words:\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.choice(len(vocab), max_words, replace=False)\n",
    "        vocab = [vocab[i] for i in indices]\n",
    "        word_vectors = word_vectors[indices]\n",
    "\n",
    "    # 使用 t-SNE 降维到 2D\n",
    "    tsne = TSNE(\n",
    "        n_components=2, random_state=seed, perplexity=75, init=\"pca\", n_iter=1000\n",
    "    )\n",
    "    reduced_embeddings = tsne.fit_transform(word_vectors)\n",
    "\n",
    "    # 绘制 t-SNE 可视化图\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    for i, word in enumerate(vocab):\n",
    "        x, y = reduced_embeddings[i]\n",
    "        plt.scatter(x, y, color=\"blue\", s=10)\n",
    "        plt.text(x + 0.2, y + 0.2, word, fontsize=8, color=\"gray\")\n",
    "    plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 4.2: Visualise the Word Representations of Biomedical Entities by t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 Word2Vec 模型的词向量\n",
    "def Exploration_2(model, BME=kimi_BME, max_words=1000, seed=1022):\n",
    "    random.seed(seed)\n",
    "    # 提取词汇表和对应的词向量\n",
    "    BMEs = [word for words in list(BME.values()) for word in words]\n",
    "    BME_classes = [c for c in list(BME.keys())]\n",
    "    vocab = model.wv.index_to_key\n",
    "    vocab_1 = list(filter(lambda x: x in BMEs, vocab))\n",
    "    vocab_2 = sample(vocab, max_words - len(vocab_1))\n",
    "\n",
    "    vocab = vocab_1 + vocab_2  # 提取词汇表\n",
    "    word_vectors = model.wv[vocab]  # 提取词向量\n",
    "\n",
    "    # 限制最多可视化的词数量\n",
    "    if len(vocab) > max_words:\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.choice(len(vocab), max_words, replace=False)\n",
    "        vocab = [vocab[i] for i in indices]\n",
    "        word_vectors = word_vectors[indices]\n",
    "\n",
    "    # 使用 t-SNE 降维到 2D\n",
    "    tsne = TSNE(\n",
    "        n_components=2, random_state=seed, perplexity=75, init=\"pca\", n_iter=1000\n",
    "    )\n",
    "    reduced_embeddings = tsne.fit_transform(word_vectors)\n",
    "\n",
    "    # 绘制 t-SNE 可视化图\n",
    "    colors = [\n",
    "        \"pink\",\n",
    "        \"blue\",\n",
    "        \"green\",\n",
    "        \"yellow\",\n",
    "        \"purple\",\n",
    "        \"magenta\",\n",
    "        \"red\",\n",
    "        \"orange\",\n",
    "        \"cyan\",\n",
    "        \"lawngreen\",\n",
    "        \"olive\",\n",
    "        \"lightblue\",\n",
    "    ]\n",
    "    colors = dict(zip(BME_classes, colors[: len(BME_classes)]))\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    l = {}\n",
    "    for i, word in enumerate(vocab):\n",
    "        x, y = reduced_embeddings[i]\n",
    "        for c in BME_classes:\n",
    "            if word in BME[c]:\n",
    "                color = colors[c]\n",
    "                size = 30\n",
    "                l[c] = (x, y, color)\n",
    "                break\n",
    "            else:\n",
    "                color = \"gray\"\n",
    "                size = 5\n",
    "        plt.scatter(x, y, color=color, s=size)\n",
    "        plt.text(x + 0.2, y + 0.2, word, fontsize=0.3 * size, color=color)\n",
    "    for key, value in l.items():\n",
    "        plt.scatter(value[0], value[1], color=value[2], s=size, label=key)\n",
    "    plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 4.3: Co-occurrence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exploration_3(\n",
    "    model, target_word=\"coronavirus\", k=10, output_file=\".\\cooccurrence_words.xlsx\"\n",
    "):\n",
    "    if target_word in model.vocab:\n",
    "        # Get similar words\n",
    "        cooccurrence_words = model.get_similar_words(target_word, k)\n",
    "        print(\"Top 10 similar words:\")\n",
    "        for word, similarity in cooccurrence_words:\n",
    "            print(\"{0}: {1:.5f}\".format(word, similarity))\n",
    "\n",
    "        # Save to Excel\n",
    "        df = pd.DataFrame(cooccurrence_words, columns=[\"Word\", \"Similarity\"])\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"Co-occurrence words saved to {output_file}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"{target_word} not in vocabulary.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track 4.4: Semantic Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exploration_4(\n",
    "    model, target_word=\"coronavirus\", output_file=\".\\semantic_similar_words.xlsx\"\n",
    "):\n",
    "    if target_word.lower() in model.wv:\n",
    "        target_vector = model.wv[target_word.lower()]\n",
    "        all_words = model.wv.index_to_key\n",
    "        similarities = []\n",
    "\n",
    "        for word in all_words:\n",
    "            if word != target_word.lower():\n",
    "                similarity = cosine_similarity([target_vector], [model.wv[word]])[0][0]\n",
    "                similarities.append((word, similarity))\n",
    "\n",
    "        sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "        top_similar_words = sorted_similarities[:10]\n",
    "\n",
    "        # Save to Excel\n",
    "        df = pd.DataFrame(top_similar_words, columns=[\"Word\", \"Similarity\"])\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"Top similar words saved to {output_file}\")\n",
    "\n",
    "        print(\"Top 10 similar words:\")\n",
    "        for word, similarity in top_similar_words:\n",
    "            print(\"{0}:{1:.5f}\".format(word, similarity))\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        print(f\"{target_word.lower()} not in vocabulary.\")\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (Bonus +10%): Open Challenge: Mining Biomedical Knowledge\n",
    "\n",
    "A fundamental task in clinical/biomedical natural language processing is to extract intelligence from biomedical text corpus automatically and efficiently. More specifically, the intelligence may include biomedical entities mentioned in text, relations between biomedical entities, clinical features of patients, progression of diseases, all of which can be used to predict, understand and improve patients' outcomes.\n",
    "\n",
    "This open challenge is to build a biomedical knowledge graph based on the CORD-19 dataset and mine useful information from it. We recommend the following steps but you are also encouraged to develop your solution from scratch.\n",
    "\n",
    "### Extract Biomedical Entities from Text\n",
    "\n",
    "Extract biomedical entities (such as fever, cough, headache, lung cancer, heart attack) from text. Note that:\n",
    "\n",
    "- The biomedical entities may consist of multiple words. For example, heart attack, multiple myeloma etc.\n",
    "- The biomedical entities may be written in synoynms. For example, low blood pressure for hypotension.\n",
    "- The biomedical entities may be written in different forms. For example, smoking, smokes, smoked.\n",
    "\n",
    "### Extract Relations between Biomedical Entities\n",
    "\n",
    "Extract relations between biomedical entities based on their appearance in text. You may define a relation between biomedical entities by one or more of the following criteria:\n",
    "\n",
    "- The biomedical entities frequentely co-occuer together.\n",
    "- The biomedical entities have similar word representations.\n",
    "- The biomedical entities have clear relations based on textual narratives. For example, \"The most common symptoms for COVID-19 are fever and cough\" so we know there are relations between \"COVID-19\", \"fever\" and \"cough\".\n",
    "\n",
    "### Build a Biomedical Knowledge Graph of COVID-19\n",
    "\n",
    "Build a knoweledge graph based on the results from track 5.1 and 5.2 and visualise it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "\n",
    "###################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
